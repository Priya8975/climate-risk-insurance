{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Uninsurability Risk Classification\n",
    "\n",
    "ML classifiers predicting which US counties face insurance affordability crises using a composite risk target derived from claim severity, disaster exposure, FEMA Housing Assistance, and aggregate claims.\n",
    "\n",
    "1. **Gradient Boosting Classifier** — primary tree-based ensemble model\n",
    "2. **Random Forest Classifier** — bagging-based comparison model\n",
    "3. **SHAP Analysis** — feature importance and interaction effects\n",
    "\n",
    "**Data:** County-year panel (2004–2024), 25,760 observations across 3,240 US counties.  \n",
    "**Target:** Composite uninsurability risk (~18% positive rate) based on high claim severity, cumulative disaster exposure, FEMA Housing Assistance damage, and high total claims paid.  \n",
    "**Features:** 32 predictive features (disaster exposure, demographics, macro indicators, lagged claims) — no current-year claims data used, to ensure genuine out-of-sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import shap\n",
    "\n",
    "from src.utils.config import MODELS_DIR, REPORTS_FIGURES, DATA_PROCESSED\n",
    "\n",
    "REPORTS_FIGURES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved results\n",
    "comparison = pd.read_csv(MODELS_DIR / \"classifier_comparison_metrics.csv\")\n",
    "feature_importance = pd.read_csv(MODELS_DIR / \"classifier_feature_importance.csv\")\n",
    "shap_importance = pd.read_csv(MODELS_DIR / \"classifier_shap_importance.csv\")\n",
    "roc_data = pd.read_csv(MODELS_DIR / \"classifier_roc_curves.csv\")\n",
    "predictions = pd.read_csv(MODELS_DIR / \"classifier_predictions.csv\", dtype={\"county_fips\": str})\n",
    "cv_results = pd.read_csv(MODELS_DIR / \"classifier_cv_results.csv\")\n",
    "risk_scores = pd.read_csv(MODELS_DIR / \"classifier_risk_scores.csv\", dtype={\"county_fips\": str})\n",
    "thresholds = pd.read_csv(MODELS_DIR / \"classifier_target_thresholds.csv\")\n",
    "feature_names = pd.read_csv(MODELS_DIR / \"classifier_feature_names.csv\")[\"feature\"].tolist()\n",
    "\n",
    "# Load SHAP explanation\n",
    "with open(MODELS_DIR / \"classifier_shap_values.pkl\", \"rb\") as f:\n",
    "    shap_explanation = pickle.load(f)\n",
    "\n",
    "# Load panel for context\n",
    "panel = pd.read_csv(DATA_PROCESSED / \"county_year_panel_glm_ready.csv\", dtype={\"county_fips\": str})\n",
    "\n",
    "print(f\"Panel dataset: {panel.shape}\")\n",
    "print(f\"Features used: {len(feature_names)}\")\n",
    "print(f\"Counties scored: {len(risk_scores):,}\")\n",
    "print(f\"\\n=== Model Comparison ===\")\n",
    "display(comparison.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable: Composite Uninsurability Risk\n",
    "\n",
    "The target is constructed from four risk signals (thresholds computed from training data only to prevent leakage):\n",
    "\n",
    "1. **High claim severity** — avg_claim_severity >= 75th percentile of positive claims\n",
    "2. **High cumulative disasters** — cum_disasters_3yr >= 75th percentile\n",
    "3. **FEMA Housing Assistance damage** — ha_total_damage > $0\n",
    "4. **High total claims paid** — total_claims_paid >= 75th percentile\n",
    "\n",
    "A county-year is labeled **high risk** if it meets **>= 2 of 4** signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display thresholds\n",
    "print(\"Target Construction Thresholds (from training data <= 2021):\")\n",
    "print(f\"  Claim severity:     >= ${thresholds['severity_threshold'].values[0]:,.0f}\")\n",
    "print(f\"  Cum disasters 3yr:  >= {thresholds['disaster_threshold'].values[0]:.1f}\")\n",
    "print(f\"  Total claims paid:  >= ${thresholds['claims_paid_threshold'].values[0]:,.0f}\")\n",
    "print(f\"  HA damage:          > $0\")\n",
    "print(f\"  Min signals needed: {int(thresholds['min_signals'].values[0])}\")\n",
    "\n",
    "# Recreate target for visualization\n",
    "sev_thr = thresholds['severity_threshold'].values[0]\n",
    "dis_thr = thresholds['disaster_threshold'].values[0]\n",
    "cp_thr = thresholds['claims_paid_threshold'].values[0]\n",
    "\n",
    "panel['sig_severity'] = (panel['avg_claim_severity'] >= sev_thr).astype(int)\n",
    "panel['sig_disasters'] = (panel['cum_disasters_3yr'] >= dis_thr).astype(int)\n",
    "panel['sig_ha'] = (panel['ha_total_damage'] > 0).astype(int)\n",
    "panel['sig_claims'] = (panel['total_claims_paid'] >= cp_thr).astype(int)\n",
    "panel['risk_signals'] = panel['sig_severity'] + panel['sig_disasters'] + panel['sig_ha'] + panel['sig_claims']\n",
    "panel['uninsurability_risk'] = (panel['risk_signals'] >= 2).astype(int)\n",
    "\n",
    "# Signal rates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Signal positive rates\n",
    "signals = {\n",
    "    'High Claim\\nSeverity': panel['sig_severity'].mean(),\n",
    "    'High Cumulative\\nDisasters': panel['sig_disasters'].mean(),\n",
    "    'FEMA HA\\nDamage': panel['sig_ha'].mean(),\n",
    "    'High Total\\nClaims Paid': panel['sig_claims'].mean(),\n",
    "}\n",
    "colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71']\n",
    "axes[0].bar(signals.keys(), [v * 100 for v in signals.values()], color=colors, edgecolor='white')\n",
    "axes[0].set_ylabel('Positive Rate (%)')\n",
    "axes[0].set_title('Risk Signal Positive Rates')\n",
    "for i, v in enumerate(signals.values()):\n",
    "    axes[0].text(i, v * 100 + 0.5, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Train vs test target distribution\n",
    "train_rate = panel[panel['year'] <= 2021]['uninsurability_risk'].mean()\n",
    "test_rate = panel[panel['year'] > 2021]['uninsurability_risk'].mean()\n",
    "bars = axes[1].bar(['Train (2004-2021)', 'Test (2022-2024)'], \n",
    "                    [train_rate * 100, test_rate * 100],\n",
    "                    color=['#3498db', '#e74c3c'], edgecolor='white')\n",
    "axes[1].set_ylabel('High Risk Rate (%)')\n",
    "axes[1].set_title('Uninsurability Risk Rate: Train vs Test')\n",
    "for bar, rate in zip(bars, [train_rate, test_rate]):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                 f'{rate:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"uninsurability_target_distribution.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall positive rate: {panel['uninsurability_risk'].mean():.1%}\")\n",
    "print(f\"Train positive rate:   {train_rate:.1%}\")\n",
    "print(f\"Test positive rate:    {test_rate:.1%}\")\n",
    "print(f\"\\nThe higher test rate reflects accelerating climate risk — consistent with Module 1 findings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatted comparison table\n",
    "comp_display = comparison.copy()\n",
    "comp_display['cv_auc'] = comp_display.apply(\n",
    "    lambda r: f\"{r['cv_auc_mean']:.4f} \\u00b1 {r['cv_auc_std']:.4f}\", axis=1\n",
    ")\n",
    "comp_display['cv_ap'] = comp_display.apply(\n",
    "    lambda r: f\"{r['cv_ap_mean']:.4f} \\u00b1 {r['cv_ap_std']:.4f}\", axis=1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON: Gradient Boosting vs Random Forest\")\n",
    "print(\"=\"*70)\n",
    "display(\n",
    "    comp_display[['model', 'auc_roc', 'avg_precision', 'f1_score', \n",
    "                   'brier_score', 'cv_auc', 'cv_ap']]\n",
    "    .set_index('model')\n",
    "    .rename(columns={\n",
    "        'auc_roc': 'AUC-ROC (Test)',\n",
    "        'avg_precision': 'Avg Precision (Test)',\n",
    "        'f1_score': 'F1 Score (Test)',\n",
    "        'brier_score': 'Brier Score (Test)',\n",
    "        'cv_auc': '5-Fold CV AUC-ROC',\n",
    "        'cv_ap': '5-Fold CV Avg Precision',\n",
    "    })\n",
    "    .T\n",
    ")\n",
    "\n",
    "best_model = comparison.loc[comparison['auc_roc'].idxmax(), 'model']\n",
    "print(f\"\\nBest model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "gb_auc = comparison[comparison['model'] == 'Gradient Boosting']['auc_roc'].values[0]\n",
    "rf_auc = comparison[comparison['model'] == 'Random Forest']['auc_roc'].values[0]\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(roc_data['gb_fpr'].dropna(), roc_data['gb_tpr'].dropna(),\n",
    "             color='#e74c3c', linewidth=2, label=f'Gradient Boosting (AUC={gb_auc:.3f})')\n",
    "axes[0].plot(roc_data['rf_fpr'].dropna(), roc_data['rf_tpr'].dropna(),\n",
    "             color='#3498db', linewidth=2, label=f'Random Forest (AUC={rf_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC=0.500)')\n",
    "axes[0].fill_between(roc_data['gb_fpr'].dropna(), roc_data['gb_tpr'].dropna(), alpha=0.05, color='#e74c3c')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve: Uninsurability Risk Classification')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "gb_prec, gb_rec, _ = precision_recall_curve(predictions['y_test'], predictions['y_prob_gb'])\n",
    "rf_prec, rf_rec, _ = precision_recall_curve(predictions['y_test'], predictions['y_prob_rf'])\n",
    "gb_ap = average_precision_score(predictions['y_test'], predictions['y_prob_gb'])\n",
    "rf_ap = average_precision_score(predictions['y_test'], predictions['y_prob_rf'])\n",
    "\n",
    "axes[1].plot(gb_rec, gb_prec, color='#e74c3c', linewidth=2, label=f'Gradient Boosting (AP={gb_ap:.3f})')\n",
    "axes[1].plot(rf_rec, rf_prec, color='#3498db', linewidth=2, label=f'Random Forest (AP={rf_ap:.3f})')\n",
    "baseline = predictions['y_test'].mean()\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', linewidth=1, alpha=0.5, label=f'Baseline={baseline:.3f}')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"classifier_roc_pr_curves.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, prob_col, name, color in [\n",
    "    (axes[0], 'y_prob_gb', 'Gradient Boosting', 'Reds'),\n",
    "    (axes[1], 'y_prob_rf', 'Random Forest', 'Blues'),\n",
    "]:\n",
    "    threshold = comparison[comparison['model'] == name]['optimal_threshold'].values[0]\n",
    "    y_pred = (predictions[prob_col] >= threshold).astype(int)\n",
    "    cm = confusion_matrix(predictions['y_test'], y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=color, ax=ax,\n",
    "                xticklabels=['Low Risk', 'High Risk'],\n",
    "                yticklabels=['Low Risk', 'High Risk'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'{name}\\n(threshold={threshold:.3f})')\n",
    "\n",
    "plt.suptitle('Confusion Matrices (Optimal Threshold via Youden\\'s J)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"classifier_confusion_matrices.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (sklearn Impurity-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_top = feature_importance.nlargest(top_n, 'gb_importance')\n",
    "axes[0].barh(range(top_n), gb_top['gb_importance'].values[::-1], color='#e74c3c', alpha=0.8)\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(gb_top['feature'].values[::-1])\n",
    "axes[0].set_xlabel('Feature Importance')\n",
    "axes[0].set_title('Gradient Boosting — Top 15 Features')\n",
    "\n",
    "# Random Forest\n",
    "rf_top = feature_importance.nlargest(top_n, 'rf_importance')\n",
    "axes[1].barh(range(top_n), rf_top['rf_importance'].values[::-1], color='#3498db', alpha=0.8)\n",
    "axes[1].set_yticks(range(top_n))\n",
    "axes[1].set_yticklabels(rf_top['feature'].values[::-1])\n",
    "axes[1].set_xlabel('Feature Importance')\n",
    "axes[1].set_title('Random Forest — Top 15 Features')\n",
    "\n",
    "plt.suptitle('sklearn Feature Importance (Impurity-Based)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"classifier_feature_importance.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Analysis: Global Feature Importance\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides model-agnostic feature importance based on game theory. Unlike sklearn's impurity-based importance, SHAP values show both the **magnitude** and **direction** of each feature's contribution to individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP bar plot (mean |SHAP value| per feature)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_shap = shap_importance.head(20)\n",
    "colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(top_shap)))\n",
    "\n",
    "ax.barh(range(len(top_shap)), top_shap['mean_abs_shap'].values[::-1], \n",
    "        color=colors[::-1], edgecolor='white')\n",
    "ax.set_yticks(range(len(top_shap)))\n",
    "ax.set_yticklabels(top_shap['feature'].values[::-1])\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_title('SHAP Feature Importance: Top 20 Risk Drivers\\n(Gradient Boosting Classifier)', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"shap_bar_plot.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Risk Drivers (SHAP):\")\n",
    "for _, row in top_shap.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: mean |SHAP| = {row['mean_abs_shap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SHAP Beeswarm Plot\n",
    "\n",
    "Each dot is one county-year observation. The x-axis shows the SHAP value (how much that feature pushed the prediction toward high risk or low risk). The color shows the feature's actual value (red = high, blue = low)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "shap.plots.beeswarm(shap_explanation, max_display=20, show=False)\n",
    "plt.title('SHAP Beeswarm Plot: Feature Impact on Uninsurability Risk\\n(Gradient Boosting)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"shap_beeswarm_plot.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SHAP Dependence Plots\n",
    "\n",
    "How the top 4 features influence risk predictions across their value ranges. SHAP automatically selects the best interaction feature (colored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_4_features = shap_importance.head(4)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (feature, ax) in enumerate(zip(top_4_features, axes.flatten())):\n",
    "    feat_idx = feature_names.index(feature)\n",
    "    shap.plots.scatter(shap_explanation[:, feature], ax=ax, show=False)\n",
    "    ax.set_title(f'{feature}', fontsize=11)\n",
    "\n",
    "plt.suptitle('SHAP Dependence Plots: Top 4 Risk Drivers', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"shap_dependence_plots.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Top At-Risk Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge risk scores with panel context\n",
    "county_context = (\n",
    "    panel.groupby(['county_fips', 'state'])\n",
    "    .agg(\n",
    "        mean_severity=('avg_claim_severity', 'mean'),\n",
    "        total_disasters=('total_disasters', 'sum'),\n",
    "        total_claims=('claim_count', 'sum'),\n",
    "        population=('total_population', 'mean'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "top_risk = risk_scores.head(25).merge(county_context, on=['county_fips', 'state'], how='left')\n",
    "\n",
    "print(\"Top 25 Highest-Risk Counties (by ML Risk Score)\")\n",
    "print(\"=\" * 90)\n",
    "display(\n",
    "    top_risk[['risk_rank', 'county_fips', 'state', 'mean_risk', 'max_risk',\n",
    "              'total_disasters', 'mean_severity', 'total_claims', 'population']]\n",
    "    .rename(columns={\n",
    "        'risk_rank': 'Rank',\n",
    "        'county_fips': 'FIPS',\n",
    "        'state': 'State',\n",
    "        'mean_risk': 'Mean Risk',\n",
    "        'max_risk': 'Max Risk',\n",
    "        'total_disasters': 'Disasters',\n",
    "        'mean_severity': 'Avg Severity ($)',\n",
    "        'total_claims': 'Total Claims',\n",
    "        'population': 'Population',\n",
    "    })\n",
    "    .round(2)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# State summary\n",
    "print(\"\\nState Distribution of Top 25:\")\n",
    "print(top_risk['state'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Geographic Visualization: County Risk Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    risk_scores,\n",
    "    geojson=\"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\",\n",
    "    locations=\"county_fips\",\n",
    "    color=\"mean_risk\",\n",
    "    color_continuous_scale=\"YlOrRd\",\n",
    "    scope=\"usa\",\n",
    "    title=\"Uninsurability Risk Score by County (ML Model Prediction)\",\n",
    "    labels={\"mean_risk\": \"Risk Score\"},\n",
    "    range_color=[0, risk_scores['mean_risk'].quantile(0.95)],\n",
    ")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0}, width=1000, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Risk Score Distribution by FEMA Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map counties to FEMA regions\n",
    "from src.utils.config import FEMA_REGIONS\n",
    "\n",
    "state_to_region = {}\n",
    "for region, states in FEMA_REGIONS.items():\n",
    "    for s in states:\n",
    "        state_to_region[s] = region\n",
    "\n",
    "risk_with_region = risk_scores.copy()\n",
    "risk_with_region['state_fips'] = risk_with_region['county_fips'].str[:2]\n",
    "risk_with_region['fema_region'] = risk_with_region['state_fips'].map(state_to_region)\n",
    "\n",
    "region_labels = {\n",
    "    1: \"R1: New England\",\n",
    "    2: \"R2: NJ,NY\",\n",
    "    3: \"R3: Mid-Atlantic\",\n",
    "    4: \"R4: Southeast\",\n",
    "    5: \"R5: Great Lakes\",\n",
    "    6: \"R6: South-Central\",\n",
    "    7: \"R7: Plains\",\n",
    "    8: \"R8: Mountain\",\n",
    "    9: \"R9: Pacific\",\n",
    "    10: \"R10: Northwest\",\n",
    "}\n",
    "risk_with_region['region_label'] = risk_with_region['fema_region'].map(region_labels)\n",
    "risk_with_region = risk_with_region.dropna(subset=['region_label'])\n",
    "\n",
    "# Box plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "order = (risk_with_region.groupby('region_label')['mean_risk']\n",
    "         .median().sort_values(ascending=False).index)\n",
    "\n",
    "sns.boxplot(data=risk_with_region, x='region_label', y='mean_risk', order=order,\n",
    "            palette='YlOrRd_r', ax=ax)\n",
    "ax.set_xlabel('FEMA Region')\n",
    "ax.set_ylabel('Mean Risk Score')\n",
    "ax.set_title('Uninsurability Risk Score Distribution by FEMA Region', fontsize=13)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"risk_by_fema_region.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Regional summary\n",
    "region_summary = (risk_with_region.groupby('region_label')['mean_risk']\n",
    "                  .agg(['mean', 'median', 'count'])\n",
    "                  .sort_values('mean', ascending=False)\n",
    "                  .round(4))\n",
    "print(\"\\nRegional Risk Summary:\")\n",
    "display(region_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5-Fold Stratified Cross-Validation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in ['Gradient Boosting', 'Random Forest']:\n",
    "    model_cv = cv_results[cv_results['model'] == model_name]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    display(model_cv[['fold', 'auc_roc', 'avg_precision', 'f1_score']].round(4))\n",
    "    print(f\"  Mean AUC-ROC:       {model_cv['auc_roc'].mean():.4f} +/- {model_cv['auc_roc'].std():.4f}\")\n",
    "    print(f\"  Mean Avg Precision: {model_cv['avg_precision'].mean():.4f} +/- {model_cv['avg_precision'].std():.4f}\")\n",
    "    print(f\"  Mean F1 Score:      {model_cv['f1_score'].mean():.4f} +/- {model_cv['f1_score'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Predicted Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "low_risk = predictions[predictions['y_test'] == 0]['y_prob_gb']\n",
    "high_risk = predictions[predictions['y_test'] == 1]['y_prob_gb']\n",
    "\n",
    "ax.hist(low_risk, bins=50, alpha=0.6, color='#3498db', label=f'Low Risk (n={len(low_risk)})')\n",
    "ax.hist(high_risk, bins=50, alpha=0.6, color='#e74c3c', label=f'High Risk (n={len(high_risk)})')\n",
    "\n",
    "threshold = comparison[comparison['model'] == 'Gradient Boosting']['optimal_threshold'].values[0]\n",
    "ax.axvline(x=threshold, color='black', linestyle='--', linewidth=2,\n",
    "           label=f'Optimal threshold = {threshold:.3f}')\n",
    "\n",
    "ax.set_xlabel('Predicted Risk Probability')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Predicted Risk Probabilities (Gradient Boosting)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"classifier_prob_distribution.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Risk Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual risk rate\n",
    "annual_risk = (\n",
    "    panel.groupby('year')\n",
    "    .agg(\n",
    "        risk_rate=('uninsurability_risk', 'mean'),\n",
    "        n_high_risk=('uninsurability_risk', 'sum'),\n",
    "        n_counties=('county_fips', 'nunique'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Risk rate trend\n",
    "axes[0].plot(annual_risk['year'], annual_risk['risk_rate'] * 100, 'o-', \n",
    "             color='#e74c3c', linewidth=2)\n",
    "axes[0].axvline(x=2021.5, color='gray', linestyle='--', alpha=0.5, label='Train/Test split')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('High Risk Rate (%)')\n",
    "axes[0].set_title('Uninsurability Risk Rate Over Time')\n",
    "axes[0].legend()\n",
    "\n",
    "# Number of high-risk counties\n",
    "axes[1].bar(annual_risk['year'], annual_risk['n_high_risk'], color='#e74c3c', alpha=0.7)\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.plot(annual_risk['year'], annual_risk['n_counties'], 'bo-', linewidth=2, label='Total counties')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('High-Risk County-Years', color='#e74c3c')\n",
    "ax2.set_ylabel('Total Counties', color='blue')\n",
    "axes[1].set_title('High-Risk Counties Over Time')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / \"risk_trends_over_time.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Key Findings Summary\n",
    "\n",
    "### Model Performance\n",
    "- **Gradient Boosting** achieved the best results: AUC-ROC = 0.83 (test), 0.87 (5-fold CV)\n",
    "- **Random Forest** performed comparably: AUC-ROC = 0.82 (test), 0.86 (5-fold CV)\n",
    "- Both models significantly outperform random classification (AUC = 0.50)\n",
    "- Low CV variance indicates stable, generalizable models\n",
    "\n",
    "### Top Risk Drivers (SHAP Analysis)\n",
    "1. **Cumulative disaster exposure** (3-year rolling) — the strongest predictor of uninsurability risk\n",
    "2. **Total population** — larger counties face higher aggregate risk exposure\n",
    "3. **Lagged claim count** — previous year's claims are highly predictive of future risk\n",
    "4. **Average incident duration** — longer-lasting events drive more severe outcomes\n",
    "5. **Severe storm count** — frequent storms compound risk beyond individual events\n",
    "\n",
    "### Geographic Risk Patterns\n",
    "- **Louisiana** dominates the top risk rankings (7+ of top 10 counties)\n",
    "- **Florida** and **Texas** are also heavily represented\n",
    "- **FEMA Region 6** (TX, LA, AR, OK, NM) has the highest median risk — consistent with Module 2's finding of 154% higher claim severity\n",
    "- Risk is accelerating: the test period (2022-2024) shows higher positive rates than training data\n",
    "\n",
    "### Implications\n",
    "- The model identifies counties where the convergence of disaster frequency, population exposure, and historical claims creates genuine uninsurability pressure\n",
    "- Demographic and economic factors (income, home values, macro indicators) play a secondary but meaningful role\n",
    "- The geographic concentration in Gulf Coast states matches real-world insurer behavior — several major insurers have already left Florida and Louisiana markets\n",
    "- Early warning: counties with high cumulative disasters and rising claim counts should be monitored for insurer withdrawal risk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
